{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebApp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6-xUATNSoAw"
      },
      "source": [
        "Installing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaV8FllpVNlV"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LdKy-D9ZOJT"
      },
      "source": [
        "!pip install werkzeug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0__xpLA7-0v"
      },
      "source": [
        "!pip install python-docx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY4YjmsuXZBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273da69c-e9bc-4323-e55b-e8ef84025c29"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrSKHQdKddk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9285c7a8-3f2c-4808-84cc-4f9726948065"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/Resume_Screening/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Resume_Screening\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EYkh8iRSs--"
      },
      "source": [
        "Loading the model and word vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK8iCdkXqM9T"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yjlec3ndmhP"
      },
      "source": [
        "# load SGDClassifier\n",
        "with open('model.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzfiMSH5sOdW"
      },
      "source": [
        "# load vectorizer\n",
        "with open('vectorizer.pkl', 'rb') as f:\n",
        "    word_vectorizer = pickle.load(f)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2tY7Kj7S6UA"
      },
      "source": [
        "Categories "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlgF5ymJ5w0T"
      },
      "source": [
        "categories = [\"Advocate\",\"Arts\",\"Automation Testing\",\"Blockchain\",\"Business Analyst\",\"Civil Engineer\",\"Data Science\",\"Database\",\"DevOps Engineer\",\"DotNet Developer\",\"ETL Developer\",\"Electrical Engineering\",\"HR\",\"Hadoop\",\"Health and fitness\",\"Java Developer\",\"Mechanical Engineer\",\"Network Security Engineer\",\"Operations Manager\",\"PMO\",\"Python Developer\",\"SAP Developer\",\"Sales\",\"Testing\",\"Web Designing\"]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IZJej-sT1aI"
      },
      "source": [
        "Functions used in routes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jhGYQTT3idf"
      },
      "source": [
        "#fucntion to check if the given file is in the allowed extensions.\n",
        "def allowed_file(filename):\n",
        "    return '.' in filename and \\\n",
        "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k36IIwlI3mgE"
      },
      "source": [
        "#function to unzip file and load it onto google drive.\n",
        "def unzip():\n",
        "  extension = \".zip\"\n",
        "\n",
        "  for item in os.listdir(UPLOAD_FOLDER): # loop through items in dir\n",
        "      if item.endswith(extension): # check for \".zip\" extension\n",
        "          file_name = UPLOAD_FOLDER + \"/\" + item\n",
        "          zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
        "          zip_ref.extractall(UPLOAD_FOLDER) # extract file to dir\n",
        "          zip_ref.close() # close file\n",
        "          os.remove(file_name) # delete zipped file"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al9BUZxe3qB5"
      },
      "source": [
        "#function to get the text from resumes.\n",
        "def getText(filename):\n",
        "  doc = docx.Document(filename)\n",
        "  fullText = []\n",
        "  #getting the complete text from docx file\n",
        "  for para in doc.paragraphs:\n",
        "      fullText.append(para.text)\n",
        "  return '\\n'.join(fullText)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GETJyIM3vCT"
      },
      "source": [
        "#fucntion to clean resumes.\n",
        "def cleaned_data(resume_data):\n",
        "    #removing all URLs.\n",
        "    resume_data = re.sub('http\\S+\\s*', ' ', resume_data)\n",
        "    #removing RT and cc.\n",
        "    resume_data = re.sub('RT|cc', ' ', resume_data)\n",
        "    #removing hashtags\n",
        "    resume_data = re.sub('#\\S+', '', resume_data)\n",
        "    #removing mentions\n",
        "    resume_data = re.sub('@\\S+', '  ', resume_data)\n",
        "    #removing punctuations\n",
        "    resume_data = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resume_data)\n",
        "    #removing all non-ASCII values.\n",
        "    resume_data = re.sub(r'[^\\x00-\\x7f]',r' ', resume_data)\n",
        "    #removing extra whitespaces\n",
        "    resume_data = re.sub('\\s+', ' ', resume_data) \n",
        "    #removing numbers\n",
        "    resume_data = re.sub('[0-9]+', ' ', resume_data)\n",
        "    return resume_data"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk4nB8la3yOf"
      },
      "source": [
        "#function which sends the data through model and returns required data.\n",
        "def process_folder(categories):\n",
        "  directory = '/content/gdrive/MyDrive/Resume_Screening/Kaggle/Resumes'\n",
        "  filenames = []\n",
        "  # iterate over files in that directory.\n",
        "  for filename in os.listdir(directory):\n",
        "    filenames.append(filename)\n",
        "  resumes = []\n",
        "  for i in filenames:\n",
        "    new_directory = directory+'/'+i\n",
        "    text = getText(new_directory)\n",
        "    #appending the text to a list.\n",
        "    resumes.append(text)\n",
        "\n",
        "  cleaned_resumes = []\n",
        "  new_resume_data = {}\n",
        "  pie_chart = {}\n",
        "  for count,resume in enumerate(resumes):\n",
        "    #cleaning the resume text\n",
        "    cleaned_resume = cleaned_data(resume)\n",
        "    #appending the cleaned data to a list.\n",
        "    cleaned_resumes.append(cleaned_resume) \n",
        "    #getting the feature vector for the cleaned text.\n",
        "    word_features = word_vectorizer.transform([cleaned_resume])\n",
        "    #getting the probabilities for each category.\n",
        "    probabilities = model.predict_proba(word_features)\n",
        "    #getting the top 3 predictions from the model(but they are encoded labels.)\n",
        "    best_three = np.argsort(probabilities, axis=1)[:,-3:]\n",
        "\n",
        "    #storing the required data.\n",
        "    best_three = best_three.tolist()\n",
        "    best_three_list = []\n",
        "    for i in best_three:\n",
        "      for j in i: \n",
        "      #converting the encoded labels to the actual labels.\n",
        "        best_three_list.append(categories[j])\n",
        "        if categories[j] in pie_chart:\n",
        "          pie_chart[categories[j]] += 1\n",
        "        else : \n",
        "          pie_chart[categories[j]] = 1\n",
        "\n",
        "      new_resume_data[count] = (best_three_list, resume, filenames[count])\n",
        "  return new_resume_data, pie_chart, cleaned_resumes"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5S18HNp0p3b"
      },
      "source": [
        "#function for implementing search feature\n",
        "def search(requested_categories, new_resume_data):\n",
        "  requested_data1 = []\n",
        "  requested_data2 = []\n",
        "  requested_data3 = []\n",
        "\n",
        "  required = len(requested_categories)\n",
        "  while required > 0 : \n",
        "    if required == 3 :#if all 3 categories are mentioned.\n",
        "      for i in range(0,len(new_resume_data)):\n",
        "        count = 0\n",
        "        for cat in new_resume_data[i][0]:\n",
        "          if cat in requested_categories: \n",
        "            count += 1\n",
        "        if count == required:\n",
        "          requested_data3.append([new_resume_data[i][0], new_resume_data[i][1], new_resume_data[i][2]])\n",
        "    elif required == 2 :#if only 2 categories are mentioned.\n",
        "      for i in range(0,len(new_resume_data)):\n",
        "        count = 0\n",
        "        for cat in new_resume_data[i][0]:\n",
        "          if cat in requested_categories: \n",
        "            count += 1\n",
        "\n",
        "        if count == required:\n",
        "          requested_data2.append([new_resume_data[i][0], new_resume_data[i][1], new_resume_data[i][2]])\n",
        "    else:#if only 1 category is mentioned.\n",
        "       for i in range(0,len(new_resume_data)):\n",
        "        count = 0\n",
        "        for cat in new_resume_data[i][0]:\n",
        "            if cat in requested_categories: \n",
        "              count += 1\n",
        "\n",
        "        if count == required:\n",
        "          requested_data1.append([new_resume_data[i][0], new_resume_data[i][1], new_resume_data[i][2]])\n",
        "\n",
        "    required -= 1\n",
        "\n",
        "  return requested_data1, requested_data2, requested_data3"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2YdpznD26-T"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from wordcloud import WordCloud\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QAljA305WOt"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVQa2GeO7FY9"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5Tm-jU13FSB"
      },
      "source": [
        "#function to generate wordCloud string which to used to get the word cloud.\n",
        "def wordCloud_string(cleaned_resumes):\n",
        "  #getting the complete text from the dataset.\n",
        "  corpus = \"\"\n",
        "  for i in range(0,len(cleaned_resumes)):\n",
        "    corpus = corpus + cleaned_resumes[i].lower()\n",
        "  #creating the tokenizer.\n",
        "  tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
        "  tokens = tokenizer.tokenize(corpus)\n",
        "  #getting the stop words from nltk dataset.\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  #removing the stop words from the tokens.\n",
        "  words = []\n",
        "  for token in tokens:\n",
        "      if token not in stopwords:\n",
        "          words.append(token)\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer() \n",
        "  lemmatized_words = []\n",
        "  for word in words : \n",
        "    word = lemmatizer.lemmatize(word)\n",
        "    lemmatized_words.append(word)\n",
        "\n",
        "  #converting into string\n",
        "  string = ' '.join([i for i in lemmatized_words if not i.isdigit()])\n",
        "  return string"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8-jkafB9PCD"
      },
      "source": [
        "!pip install python-docx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oukgWkeaTw76"
      },
      "source": [
        "Importing required packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpgL0NfHkPhV"
      },
      "source": [
        "import base64\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUbmFlqQTHZV",
        "outputId": "6c429b9f-612f-42a5-8536-6aaf6d4b69f5"
      },
      "source": [
        "import os,zipfile,docx\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, flash, request, redirect, url_for, render_template\n",
        "from flask import send_from_directory\n",
        "from werkzeug.utils import secure_filename\n",
        "UPLOAD_FOLDER = '/content/gdrive/MyDrive/Resume_Screening/Kaggle'\n",
        "ALLOWED_EXTENSIONS = {'zip'}\n",
        "from flask import session\n",
        "\n",
        "app = Flask(__name__, template_folder='/content/gdrive/MyDrive/Resume_Screening/web_templates', static_folder = '/content/gdrive/MyDrive/Resume_Screening/static')\n",
        "app.secret_key = \"supersecretkey\"\n",
        "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
        "run_with_ngrok(app)   #starts ngrok when the app is run\n",
        "\n",
        "#home route.\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def upload_file():\n",
        "    if request.method == 'POST':\n",
        "        # check if the post request has the file part\n",
        "        if 'file' not in request.files:\n",
        "            return '<h1>No file part</h1>'\n",
        "        file = request.files['file']\n",
        "        # If the user does not select a file, the browser submits an\n",
        "        # empty file without a filename.\n",
        "        if file.filename == '':\n",
        "            return '<h1>No file selected</h1>'\n",
        "        if file and allowed_file(file.filename):\n",
        "            filename = secure_filename(file.filename)\n",
        "            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
        "            unzip()\n",
        "            return redirect(url_for('index'))\n",
        "    return render_template('home.html')\n",
        "\n",
        "#index route -> lists all the features.\n",
        "@app.route('/index',methods=['GET', 'POST'])\n",
        "def index():\n",
        "  if request.method == 'POST':\n",
        "      if request.form['submit_button'] == 'Bar Graph':\n",
        "        return redirect(url_for('bar_graph'))\n",
        "      if request.form['submit_button'] == 'Pie Chart':\n",
        "        return redirect(url_for('pie_chart'))\n",
        "      if request.form['submit_button'] == 'Word Cloud':\n",
        "        return redirect(url_for('wordcloud'))\n",
        "      if request.form['submit_button'] == 'Search':\n",
        "        return redirect(url_for('action_select'))\n",
        "      \n",
        "  return render_template('index.html')\n",
        "\n",
        "#select route for search\n",
        "@app.route('/action_select',  methods=['GET', 'POST'])\n",
        "def action_select():\n",
        "  all_categories = [None]\n",
        "  for i in categories:\n",
        "    all_categories.append(i)\n",
        "  \n",
        "  if request.method == 'POST':\n",
        "    string1 = request.form['cat1']\n",
        "    string2 = request.form['cat2']\n",
        "    string3 = request.form['cat3']\n",
        "\n",
        "    categories_int = []\n",
        "\n",
        "    categories_int.append(int(string1))\n",
        "    categories_int.append(int(string2))\n",
        "    categories_int.append(int(string3))\n",
        "\n",
        "    requested_categories1 = []\n",
        "\n",
        "    for i in categories_int:\n",
        "      if(i == 0): \n",
        "        continue\n",
        "      requested_categories1.append(all_categories[i])\n",
        "\n",
        "    new_resumes, pc, cr = process_folder(categories)\n",
        "    req1, req2, req3 = search(requested_categories1, new_resumes)\n",
        "\n",
        "    req1_out = []\n",
        "    req2_out = []\n",
        "    req3_out = []\n",
        "\n",
        "    for i in req3:\n",
        "      req3_out.append([i[0], i[1], i[2]])\n",
        "\n",
        "    for i in req2:\n",
        "      req2_out.append([i[0], i[1], i[2]])\n",
        "\n",
        "    for i in req1:\n",
        "      req1_out.append([i[0], i[1], i[2]])\n",
        " \n",
        "    return render_template(\"skills_display.html\", allthree = req3_out, n1 = len(req3_out), onlytwo = req2_out, n2 = len(req2_out), justone = req1_out, n3 = len(req1_out))   \n",
        "\n",
        "\n",
        "  return render_template(\"select.html\", categories = all_categories, n = len(all_categories))\n",
        "\n",
        "#route for displaying pie chart.\n",
        "@app.route('/pie_chart')\n",
        "def pie_chart():\n",
        "  new_resumes, pc, cr = process_folder(categories)\n",
        "  img = io.BytesIO()\n",
        "  plt.figure(figsize=(15,15))\n",
        "  plt.pie(pc.values())\n",
        "  plt.title('Category Pie Chart')\n",
        "  plt.legend(labels = pc.keys())\n",
        "  plt.savefig(img, format='png')\n",
        "  img.seek(0)\n",
        "\n",
        "  plot_url = base64.b64encode(img.getvalue()).decode()\n",
        "  return '<img src=\"data:image/png;base64,{}\">'.format(plot_url)\n",
        "\n",
        "#route for displaying bar graph\n",
        "@app.route('/bar_graph')\n",
        "def bar_graph():\n",
        "  nr, pc, cr = process_folder(categories)\n",
        "  img = io.BytesIO()\n",
        "  plt.figure(figsize=(15,15))\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.title('Category Count')\n",
        "  keys = list(pc.keys())\n",
        "  # get values in the same order as keys, and parse percentage values\n",
        "  vals = [float(pc[k]) for k in keys]\n",
        "  ax = sns.barplot(x=keys, y=vals)\n",
        "  for i, p in enumerate(ax.patches):\n",
        "      height = p.get_height()\n",
        "      ax.text(p.get_x()+p.get_width()/2., height + 0.1, vals[i],ha=\"center\")\n",
        "  plt.savefig(img, format='png')\n",
        "  img.seek(0)\n",
        "\n",
        "  plot_url = base64.b64encode(img.getvalue()).decode()\n",
        "  return '<img src=\"data:image/png;base64,{}\">'.format(plot_url)\n",
        "\n",
        "#route for displaying word cloud.\n",
        "@app.route('/wordcloud')\n",
        "def wordcloud():\n",
        "  nr, pc, cleaned_resumes = process_folder(categories)\n",
        "  output_str = wordCloud_string(cleaned_resumes)\n",
        "  img = io.BytesIO()\n",
        "  plt.subplots(figsize=(15,15))\n",
        "  wordcloud = WordCloud(background_color = 'black', max_words = 100, width = 1500, height = 1500).generate(output_str)\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.title('Resume Text WordCloud (100 Words)')\n",
        "  plt.axis('off')\n",
        "  plt.savefig(img, format='png')\n",
        "  img.seek(0)\n",
        "\n",
        "  plot_url = base64.b64encode(img.getvalue()).decode()\n",
        "  return '<img src=\"data:image/png;base64,{}\">'.format(plot_url)\n",
        "\n",
        "app.run()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://3a17-35-185-253-84.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}